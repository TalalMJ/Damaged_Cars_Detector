{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:25:43.560890Z",
     "start_time": "2019-05-09T20:25:34.439361Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ipSpFLaCBtAj"
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "# see https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%pylab inline\n",
    "\n",
    "# sets backend to render higher res images\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "#######################\n",
    "#       imports       #\n",
    "#######################\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "# !wget http://soph.info/metis/soph.py\n",
    "from soph import plot_decision_boundary\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYGM9x9wBtAt"
   },
   "source": [
    "# An Introduction to Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFfsGhovBtAw"
   },
   "source": [
    "### This notebook\n",
    "\n",
    "In this notebook, we'll cover the two important \"ingredients\" of Neural Networks:\n",
    "1. Layers\n",
    "2. Activation functions\n",
    "\n",
    "Note: We will use the keras library for demonstration, but it's not meant to be explained here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLnP0iAzBtAy"
   },
   "source": [
    "## XOR\n",
    "\n",
    "The XOR problem is a classic problem in machine learning. This problem asks a model to learn the XOR logical function reproduced below\n",
    "\n",
    "X | Y | XOR\n",
    "---|---|---\n",
    "0 | 0 | 0\n",
    "1 | 0 | 1\n",
    "0 | 1 | 1\n",
    "1 | 1 | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbeO9wSWBtA3"
   },
   "source": [
    "Question: Do you think this could be solved with logistic regression? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xmQqAZBoBtA4"
   },
   "source": [
    "## Back up: revisit the problem of nonlinear data.\n",
    "\n",
    "To get started discussing neural nets, we'll look at a very old question in Machine Learning: how do we get our models to learn nonlinear functions?\n",
    "\n",
    "The data below is a cluster version of the XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:25:50.559665Z",
     "start_time": "2019-05-09T20:25:49.629863Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Om0MdxYbBtA6"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "pal = dict(enumerate(sns.color_palette(\"husl\", 4)))\n",
    "\n",
    "x, y = datasets.make_blobs(n_samples=1000, n_features=2,\n",
    "                           centers=[[0, 0], [0, 1], [1, 0],\n",
    "                                    [1, 1]], cluster_std=.1, random_state=0)\n",
    "\n",
    "blob_df = pd.DataFrame({\"x0\": x[:, 0], \"x1\": x[:, 1], \"y\": y})\n",
    "\n",
    "blob_df[\"y_non\"] = blob_df.y.isin([1, 2]).astype(\"int\")\n",
    "\n",
    "sns.scatterplot(x=\"x0\", y=\"x1\", hue=\"y_non\", data=blob_df, alpha=.2,\n",
    "                edgecolor=None, palette=pal)\n",
    "title(\"Non-Separable clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:25:51.074620Z",
     "start_time": "2019-05-09T20:25:51.065664Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MzWARabGBtBB"
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Cu_uX9hBtBG"
   },
   "source": [
    "## Building a model in keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXpoCbJuBtBH"
   },
   "source": [
    "Let's learn how Keras works by building something we already know: Logistic Regression.\n",
    "\n",
    "Recall that Logistic Regression uses the same linear operation we've become very familiar with:\n",
    "$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b$\n",
    "\n",
    "Logistic Regression uses the sigmoid function to transform the output of the above equation to a probability, $p(\\hat{y} = 1 | \\mathbf{x}) = \\sigma( \\mathbf{w}^\\top \\mathbf{x} + b)$. \n",
    "\n",
    "Below, we will use the building blocks provided by Keras to construct a Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7t2w28-DBtBI"
   },
   "source": [
    "Okay, let's talk about the code we'll use to build this model in keras:\n",
    "\n",
    "The most common method we'll use to build models is `keras.Sequential`. Which works for any model where each layer flows directly into the next one.  \n",
    "```python\n",
    "logreg_keras = keras.Sequential([...])\n",
    "```\n",
    "\n",
    "The input to `keras.Sequential` is a list of layers. More details on layers in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4bN5cXmtBtBJ"
   },
   "source": [
    "In keras, models are built piece-by-piece using layers. Each layer performs one step of operations.\n",
    "\n",
    "The operation we want to perform for Logistic Regression is $\\mathbf{w}^\\top \\mathbf{x} + b$. That linear operation is performed by a dense layer: `keras.layers.Dense`.\n",
    "\n",
    "```python\n",
    "logreg_keras = keras.Sequential([\n",
    "    keras.layers.Dense(...),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DeKrqLCrBtBL"
   },
   "source": [
    "When keras builds a model, it initializes all the necessary variables before running the model. In our case, these are $\\mathbf{w}$ and $b$. In order to initialize these, keras must know their dimensionality (in other words, how big they are). \n",
    "\n",
    "Keras calculates the dimensionality from the size of the input and output. Take our example above: we have two input features, $x_1$ and $x_2$ and we want one output, $\\hat{y}$. From that we can infer that we need $\\mathbf{w}$ to have the shape $[2 \\times 1]$ and $b$ to have the shape $[1]$. This is exactly how keras does it when we add our layer.\n",
    "\n",
    "We'll add the `units=1` argument for the output size and the `input_shape=(2,)` for the input. \n",
    "```python\n",
    "logreg_keras = keras.Sequential([\n",
    "    keras.layers.Dense(units=1, input_shape=(2, )),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1TcQ5v2BtBM"
   },
   "source": [
    "The next step is to tell keras how to train the model using `logreg_keras.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\")`.\n",
    "\n",
    "We'll use `optimizer=\"sgd\"` to tell keras to use vanilla stochastic gradient descent when searching for the best $\\mathbf{w}$ and $b$. \n",
    "\n",
    "The loss function measures how well we're doing. Binary Cross-Entropy is a loss function that works when we're predicting probabilities for a binary outcome. BCE reduces to zero when we predict 100% for every example labeled true and 0% for every example labeled false. BCE increases as we travel away from that ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MthCHB9xBtBN"
   },
   "source": [
    "Finally, we'll train the model with \n",
    "\n",
    "```python\n",
    "logreg_keras.fit(x=blob_df[[\"x0\", \"x1\"]], y=blob_df[\"y_non\"], epochs=20)\n",
    "```\n",
    "\n",
    "The new term here is **epochs**. An epoch measures the number of times a model is shown the entire training dataset. So, `epochs=20` means that keras will go through 20 iterations of showing the model the data, optimizing the model, and repeating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZU5mrtN0BtBO"
   },
   "source": [
    "Now, we're ready to train the model! Let's back up and think again about the dataset that we're using. How do we expect a vanilla Logistic Regression model to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:26:35.337706Z",
     "start_time": "2019-05-09T20:26:32.691640Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NSeOh16yBtBP"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "logreg_keras = keras.Sequential([\n",
    "    keras.layers.Dense(units=1, input_shape=(2, )),\n",
    "    keras.layers.Activation(\"sigmoid\")\n",
    "])\n",
    "\n",
    "logreg_keras.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\")\n",
    "\n",
    "logreg_keras.fit(x=blob_df[[\"x0\", \"x1\"]], y=blob_df[\"y_non\"], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:26:41.268224Z",
     "start_time": "2019-05-09T20:26:40.682249Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "9Y8dItzSBtBU"
   },
   "outputs": [],
   "source": [
    "pred_func = lambda x: logreg_keras.predict(x) > .5\n",
    "\n",
    "plot_decision_boundary(pred_func, x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "                       y=blob_df[\"y_non\"].values, points=1e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQktNCLeBtBb"
   },
   "source": [
    "## Back to Square one\n",
    "\n",
    "Okay, we're back to square one with using a linear classifier on nonlinear data. It doesn't work! \n",
    "\n",
    "**Recall**: How did we fix this for linear models previously?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZxtJOLUBtBc"
   },
   "source": [
    "Let's confirm that things like feature engineering still work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T05:04:04.473781Z",
     "start_time": "2019-05-01T05:04:04.465036Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3K7Qs5_cBtBd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:27:43.227877Z",
     "start_time": "2019-05-09T20:27:32.929669Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JGI7ZNKqBtBg"
   },
   "outputs": [],
   "source": [
    "# engineer interaction feature\n",
    "blob_df[\"x0*x1\"] = blob_df[\"x0\"]*blob_df[\"x1\"]\n",
    "\n",
    "# build model. Now our input is size 3\n",
    "logreg_keras = keras.Sequential([\n",
    "    keras.layers.Dense(units=1, input_shape=(3, )),\n",
    "    keras.layers.Activation(\"sigmoid\")\n",
    "])\n",
    "\n",
    "logreg_keras.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "logreg_keras.fit(x=blob_df[[\"x0\", \"x1\", \"x0*x1\"]], y=blob_df[\"y_non\"], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:28:01.994254Z",
     "start_time": "2019-05-09T20:28:01.376364Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "69ygSVrLBtBl"
   },
   "outputs": [],
   "source": [
    "# we have to modify the prediction function so that it can calculate our interaction feature\n",
    "pred_func = lambda x: logreg_keras.predict(np.stack((x[:,0], x[:,1], x[:,0]*x[:,1]), axis=1)) > .5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_decision_boundary(pred_func, x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "                       y=blob_df[\"y_non\"].values, points=1e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqdDP0lABtBv"
   },
   "source": [
    "Okay, so old tricks still work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yvv7ipxPBtBx"
   },
   "source": [
    "# The deep learning approach: aside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smg1IoY9BtBz"
   },
   "source": [
    "Okay, we can now use keras to build models that we've already learned. It's a bit tricker than just importing an off the shelf model from `sklearn` but we can do it. \n",
    "\n",
    "Now that we know the ropes a bit, we can start building actual *neural networks* and we can make comparisons between the traditional machine learning methods we've learned so far and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T05:36:40.426336Z",
     "start_time": "2019-05-01T05:36:40.404158Z"
    },
    "colab_type": "text",
    "id": "ZlIZ56ItBtB1"
   },
   "source": [
    "**Traditional Machine Learning**: Relatively fixed models, focus on data pre-processing. Here we might take an off-the-shelf model and then prepare the data for the model using kernels, feature extraction and engineering, and hand prep of data. With a few exceptions, the success or failure of your model will be strongly impacted by the details of your data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkZwXYHvBtB3"
   },
   "source": [
    "**Deep Learning**: Minimal data pre-processing, focus on model building. Here, we might perform some minimal data pre-processing step and then consider the data fixed. Most of our work will be setting up the model so that it *is able to* learn it's own transformation steps. \n",
    "\n",
    "In general, the deep learning approach leads to a lot more tinkering *with the structure of the model* and a lot less tinkering with your data. The power of this approach is that the processing steps are learned by the model based on the training data and and task. In many cases, this learned processing pipeline can outperform anything humans have been able to come up with by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ea4K59LsBtB4"
   },
   "source": [
    "## The deep learning approach: what's the catch?\n",
    "\n",
    "This little statement \"setting up the model so that it *is able to* learn it's own transformation steps\" is often easier said than done. \n",
    "\n",
    "In fact, in a typical deep learning project, much of the work is in finding the best architectures and modifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIEEedwmBtB5"
   },
   "source": [
    "# The deep learning approach: back to code\n",
    "\n",
    "Here you'll learn about two of the essential ingredients to any deep learning model:\n",
    "1. Layers\n",
    "2. Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHuYudBCBtB7"
   },
   "source": [
    "# Ingredient 1: Layers\n",
    "\n",
    "The first ingredient is *layers*. An insight of neural nets is that we can use the linear function several times in a sequence. In this case, every application of our linear function is called a \"layer\". Below we have $n$ layers and at the very end, we'll use $\\hat{\\mathbf{y}} = \\mathbf{h}_n$ as our estimate of our outputs:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\mathbf{w}_1^\\top \\mathbf{x} + b_1$$\n",
    "$$\\mathbf{h}_2 = \\mathbf{w}_2^\\top \\mathbf{h}_1 + b_2$$\n",
    "$$\\dots$$\n",
    "$$\\mathbf{h}_n = \\mathbf{w}_n^\\top \\mathbf{h}_{n-1} + b_n$$\n",
    "\n",
    "Now, $\\mathbf{h}_1$ is a <u>**hidden layer**</u>—a layer whose output is not used directly for prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hkwWIJFZBtB8"
   },
   "source": [
    "## Why layers?\n",
    "\n",
    "Using several layers allows the model to learn how to transform and extract valuable information from the input data and then to use that newly transformed and extracted information to perform some task, like classification or regression.  \n",
    "\n",
    "Before Deep Learning, these are steps that were typically performed ahead of time, or using kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gtb-ykryBtB9"
   },
   "source": [
    "Okay, let's revisit that non-separable blob dataset but this time, we'll see how this would be solved with deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:28:09.262555Z",
     "start_time": "2019-05-09T20:28:08.800411Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "N-58aQOlBtCA"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"x0\", y=\"x1\", hue=\"y_non\", data=blob_df, alpha=.2,\n",
    "                edgecolor=None, palette=pal)\n",
    "title(\"Non-Separable clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2L3FghvBtCK"
   },
   "source": [
    "In the code below, we're adding a second layer with 10 units. This is a relatively shallow network described by the following equation\n",
    "\n",
    "$$\\mathbf{h}_1 = \\mathbf{w}_1^\\top \\mathbf{x} + b_1$$\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{w}_2^\\top \\mathbf{h}_1 + b_2$$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:28:14.964748Z",
     "start_time": "2019-05-09T20:28:12.243980Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "J8wDzfSCBtCN"
   },
   "outputs": [],
   "source": [
    "deep_model = keras.Sequential([\n",
    "    keras.layers.Dense(units=10, input_shape=(2, )),\n",
    "    keras.layers.Dense(units=1),\n",
    "    keras.layers.Activation(\"sigmoid\")\n",
    "])\n",
    "\n",
    "deep_model.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\")\n",
    "\n",
    "deep_model.fit(x=blob_df[[\"x0\", \"x1\"]], y=blob_df[\"y_non\"], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:28:18.426662Z",
     "start_time": "2019-05-09T20:28:16.785338Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "9ocMKAGuBtCh"
   },
   "outputs": [],
   "source": [
    "pred_func = lambda x: deep_model.predict(x) > .5\n",
    "\n",
    "plot_decision_boundary(pred_func, x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "                       y=blob_df[\"y_non\"].values, points=1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GVzhv59dBtCo"
   },
   "source": [
    "As you can clearly see, layers alone are not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vO1cDvvOBtCq"
   },
   "source": [
    "## Ingredient 2: a nonlinear activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pX7as9zGBtCs"
   },
   "source": [
    "Can you spot the problem with the previous model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYjowKfqBtCv",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8f58549ab8591eb5",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "When linear functions are combined, the result is still limited to being a linear function. \n",
    "\n",
    "In other words, combining layers of linear functions will never allow our model to learn something nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anh6yCgkBtCy"
   },
   "source": [
    "We can fix this with a nonlinear activation function $f_a$.\n",
    "\n",
    "$$\\mathbf{h}_1 = f_a(\\mathbf{w}_1^\\top \\mathbf{x} + b_1)$$\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{w}_2^\\top \\mathbf{h}_1 + b_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_f1nS_pBtC0"
   },
   "source": [
    "![](img/relu.svg)\n",
    "\n",
    "Specifically, we'll use the relu activation function $f_a(\\mathbf{x}) = \\max(\\mathbf{x},0) $\n",
    "\n",
    "The addition of a nonlinear activation function allows the model to learn to process input in a nonlinear fashion, which is essential for classifying datasets like our blob dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:28:26.661365Z",
     "start_time": "2019-05-09T20:28:23.039495Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "AaC4x2tiBtC2"
   },
   "outputs": [],
   "source": [
    "deep_model = keras.Sequential([\n",
    "    keras.layers.Dense(units=10, input_shape=(2, ), name=\"hidden_layer\"),\n",
    "    keras.layers.Activation(\"tanh\", name=\"hidden_activation\"),\n",
    "    keras.layers.Dense(units=1, name=\"output_layer\"),\n",
    "    keras.layers.Activation(\"sigmoid\", name=\"sigmoid_activation\"),\n",
    "])\n",
    "\n",
    "deep_model.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "deep_model.fit(x=blob_df[[\"x0\", \"x1\"]], y=blob_df[\"y_non\"], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:28:27.485035Z",
     "start_time": "2019-05-09T20:28:26.664677Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SeDK7NypBtC-"
   },
   "outputs": [],
   "source": [
    "pred_func = lambda x: deep_model.predict(x) > .5\n",
    "\n",
    "plot_decision_boundary(pred_func, x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "                       y=blob_df[\"y_non\"].values, points=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:28:27.541753Z",
     "start_time": "2019-05-09T20:28:27.488034Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dsc9vSCtBtDE"
   },
   "outputs": [],
   "source": [
    "[arr.shape for arr in deep_model.get_layer(\"hidden_layer\").get_weights()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ai0du7y5BtDO"
   },
   "source": [
    "We did it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GctAc0jkBtDQ"
   },
   "source": [
    "How'd we do it? Let's look at a few test points to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T20:28:32.395616Z",
     "start_time": "2019-05-09T20:28:32.128893Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "O4JNXAQ4BtDR"
   },
   "outputs": [],
   "source": [
    "deep_model.save(\"img/deep_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T22:21:51.966878Z",
     "start_time": "2019-02-11T22:21:51.677087Z"
    },
    "colab_type": "text",
    "id": "9MJNX2KqBtDY"
   },
   "source": [
    "**INSTRUCTION NOTE**\n",
    "The code to make the following plot is stored in [`utils/relu_plot.ipynb`](additional-resources/utils/relu_plot.ipynb). It is beyond the scope of this lesson but students are encouraged to refer back to it for examples of how to:\n",
    "- annotate plots\n",
    "- extract outputs at different layers of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Jo0AITeBtDb"
   },
   "source": [
    "![](img/activation_function_demo.svg)\n",
    "\n",
    "Let's look closely at how our neural net accomplishes the task. \n",
    "\n",
    "- The first linear operation may not seem like much, but our model has learned a clever trick: It squishes all of the points onto a single line. Points that have similar values on each dimension (like $[1,1]$ and $[0,0]$) are pushed to the center of the line while points with lopsided values (like $[1,0]$, and $[0,1]$ are pushed to either end.\n",
    "- Normally, this alone would be no help. We still can't separate these points with a linear classifier. **However**, this is where our function relu comes in. (recall $\\text{relu}(x) = \\max(x,0)$)\n",
    "- The relu activation function pushes one leg of this line up while leaving the rest in place. A linear function wouldn't be able to select part of the line to move, but relu can!\n",
    "- Now the next layer is set up for an easy classification problem. Voila!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qj8tlKapBtDd"
   },
   "source": [
    "# Takeaways\n",
    "\n",
    "The most important points:\n",
    "- At the core of Neural Networks is our friend, the linear operation: $\\mathbf{w}^\\top\\mathbf{x} + b$\n",
    "- Two essential ingredients of Neural Nets:\n",
    "    - multiple layers\n",
    "    - nonlinear activation functions\n",
    "- Neural Nets replace hand selected pre-processing steps with transformations learned from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WruxvlIkBtDe"
   },
   "source": [
    "# Activation functions for hidden  layers\n",
    "\n",
    "\n",
    "\n",
    "| Activation function | Function | Plot | Notes | \n",
    "|:--------------------|:------|:------|:-----|\n",
    "| ReLU (Rectified Linear Unit) | $$f(x) = \\max(x, o)$$ | ![](img/relu.svg) | General purpose. Great default. |\n",
    "| tanh | $$f(x) = \\tanh(x)$$ | ![](img/tanh.svg) |  |\n",
    "| LeakyReLU | $$f(x) = \\begin{cases} \\alpha x & x < 0 \\\\ x & x \\geq 0 \\end{cases} $$ | ![](img/Leakyrelu.svg) | Useful in Generative Adversarial Networks. Cases where gradient needs to flow even in off state. |\n",
    "| ELU (Exponential Linear Unit) | $$f(x) = \\begin{cases} \\alpha e^{(x-1)} & x < 0 \\\\ x & x \\geq 0 \\end{cases} $$ |  ![](img/elu.svg) | Sometimes better for image recognition. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ehj6bk22BtDg"
   },
   "source": [
    "# Activation functions for Output layers\n",
    "\n",
    "| Activation function | Function | Plot | Notes | \n",
    "|---------------------|-------|-------|------|\n",
    "| Linear | $f(x) = x $ | ![](img/linear.svg) | Regression |\n",
    "| Sigmoid | $$f(x) = \\sigma(x) = \\frac{1}{1+e^{-x}}$$ | ![](img/sigmoid.svg) | Binary (two class) classification |\n",
    "| SoftMax | $$f_i(\\mathbf{x}) = \\frac{e^{x_i}}{ \\sum_{x_j \\in \\mathbf{x}} e^{x_j}} $$ | - | Multi-class classification | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbqcZtJWBtDh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_NN_Theory.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
